{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "933e487c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "AWS_REGION = \"us-east-1\"\n",
    "AWS_PROFILE = \"duckdb\"  # aws configure sso --profile duckdb\n",
    "\n",
    "os.environ[\"AWS_PROFILE\"] = AWS_PROFILE\n",
    "os.environ[\"AWS_REGION\"] = AWS_REGION\n",
    "os.environ[\"PULUMI_CONFIG_PASSPHRASE\"] = \"\"\n",
    "\n",
    "S3_BUCKET_NAME = \"mlops-club-datalake-stream\"\n",
    "GLUE_DATABASE_NAME = \"nyc_taxi\"\n",
    "GREEN_TAXI_TABLE_NAME = \"green_taxi_trips\"\n",
    "\n",
    "DATA_DIR = Path(\"../../src/data/green\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c42450f",
   "metadata": {},
   "source": [
    "# Create S3 Bucket\n",
    "\n",
    "Yeah... pulumi is probly overkill. Sorry. I figured it'd help me clean everything up at the end.\n",
    "\n",
    "Creates\n",
    "\n",
    "- an S3 bucket for our datalake\n",
    "- a glue database in the global AWS Glue catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09be8df6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "warning: A new version of Pulumi is available. To upgrade from version '3.162.0' to '3.175.0', run \n",
      "   $ brew update && brew upgrade pulumi\n",
      "or visit https://pulumi.com/docs/install/ for manual instructions and release notes.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1749410305.714508 1290710 fork_posix.cc:77] Other threads are currently calling into gRPC, skipping fork() handlers\n",
      "I0000 00:00:1749410311.064201 1290710 fork_posix.cc:77] Other threads are currently calling into gRPC, skipping fork() handlers\n",
      "I0000 00:00:1749410311.279778 1290710 fork_posix.cc:77] Other threads are currently calling into gRPC, skipping fork() handlers\n",
      "I0000 00:00:1749410311.479644 1290710 fork_posix.cc:77] Other threads are currently calling into gRPC, skipping fork() handlers\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket name: mlops-club-datalake-stream\n",
      "S3 Console URL: https://s3.console.aws.amazon.com/s3/buckets/mlops-club-datalake-stream?region=us-east-1\n",
      "Glue Database: nyc_taxi\n",
      "Glue Database Console URL: https://us-east-1.console.aws.amazon.com/glue/home?region=us-east-1#database:name=nyc_taxi\n"
     ]
    }
   ],
   "source": [
    "# Pulumi Automation API example: create an S3 bucket\n",
    "\n",
    "import pulumi\n",
    "import pulumi_aws as aws\n",
    "from pulumi import automation as auto\n",
    "\n",
    "def pulumi_program():\n",
    "    # The first argument is the Pulumi resource name, the second is the bucket name property\n",
    "    bucket = aws.s3.Bucket(resource_name=\"datalake-bucket\", bucket=S3_BUCKET_NAME, force_destroy=True)\n",
    "    glue_db = aws.glue.CatalogDatabase(\n",
    "        resource_name=\"glue-db\",\n",
    "        name=GLUE_DATABASE_NAME\n",
    "    )\n",
    "    pulumi.export(\"bucket_name\", bucket.id)\n",
    "    pulumi.export(\"glue_database_name\", glue_db.name)\n",
    "\n",
    "# Set up and run the Pulumi stack\n",
    "stack_name = \"dev\"\n",
    "project_name = \"s3-bucket-project\"\n",
    "\n",
    "stack = auto.create_or_select_stack(\n",
    "    stack_name=stack_name,\n",
    "    project_name=project_name,\n",
    "    program=pulumi_program,\n",
    ")\n",
    "\n",
    "stack.set_config(\"aws:region\", auto.ConfigValue(value=\"us-east-1\"))\n",
    "stack.workspace.install_plugin(\"aws\", \"v5.0.0\")\n",
    "# up_res = stack.up(on_output=print)\n",
    "up_res = stack.up()\n",
    "\n",
    "print(f\"Bucket name: {up_res.outputs['bucket_name'].value}\")\n",
    "bucket_url = f\"https://s3.console.aws.amazon.com/s3/buckets/{up_res.outputs['bucket_name'].value}?region=us-east-1\"\n",
    "print(f\"S3 Console URL: {bucket_url}\")\n",
    "print(f\"Glue Database: {up_res.outputs['glue_database_name'].value}\")\n",
    "glue_db_url = f\"https://us-east-1.console.aws.amazon.com/glue/home?region=us-east-1#database:name={up_res.outputs['glue_database_name'].value}\"\n",
    "print(f\"Glue Database Console URL: {glue_db_url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340c8d97",
   "metadata": {},
   "source": [
    "# Define our table in the database\n",
    "\n",
    "We could do this\n",
    "\n",
    "- with a `CREATE OR REPLACE TABLE` statement and DDL via athena. But then we would need to know the exact types of each column.\n",
    "- or we could have the aws data wrangler library infer this for us [(`wr.athena.to_iceberg(df, database, table)`)](https://aws-sdk-pandas.readthedocs.io/en/3.2.1/stubs/awswrangler.athena.to_iceberg.html)\n",
    "- or `pyiceberg` can create tables, since, after all, tables are the primitive of iceberg. But does this risk inefficiencies such as not declaring the partition?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17ccb73",
   "metadata": {},
   "source": [
    "### Connect to AWS Glue Catalog\n",
    "\n",
    "We would not have to connect like this if we were using AWS data wrangler's functions for creating tables.\n",
    "\n",
    "I really do not love that DS would potentially be exposed to all this. \n",
    "\n",
    "And I don't want DS thinking about which specific S3 bucket contains the underlying files for their tables ðŸ¤”.\n",
    "\n",
    "I definitely iceberg to be an implementation detail for them, and for them to think about their data simply as tables in a database like in Snowflake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89ad2358",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to glue the catalog/database via pyiceberg and credential vending\n",
    "from pyiceberg.catalog import load_catalog, Catalog\n",
    "\n",
    "# Configure catalog connection properties\n",
    "catalog_properties = {\n",
    "    \"type\": \"rest\",\n",
    "    \"uri\": f\"https://glue.{AWS_REGION}.amazonaws.com/iceberg\",\n",
    "    \"s3.region\": AWS_REGION,\n",
    "    \"rest.sigv4-enabled\": \"true\",\n",
    "    \"rest.signing-name\": \"glue\",\n",
    "    \"rest.signing-region\": AWS_REGION,\n",
    "}\n",
    "\n",
    "# presumably, this uses boto3's credential chain under the hood\n",
    "iceberg_catalog: Catalog = load_catalog(**catalog_properties)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479b0544",
   "metadata": {},
   "source": [
    "We can create the table using `pyiceberg`. I like this because `pyiceberg` can get the schema (cols and dtypes) of one of our parquet files from pyarrow.\n",
    "\n",
    "This saves us from having to hand write a DDL statement ourselves like\n",
    "\n",
    "```sql\n",
    "CREATE TABLE AwsDataCatalog.nyc_taxi.green_taxi_trips (\n",
    "    vendor_id STRING,\n",
    "    lpep_pickup_datetime TIMESTAMP,\n",
    "    lpep_dropoff_datetime TIMESTAMP,\n",
    "    passenger_count INT,\n",
    "    trip_distance DOUBLE,\n",
    "    ...\n",
    ")\n",
    "PARTITIONED BY (hour(lpep_pickup_datetime))\n",
    "STORED AS ICEBERG\n",
    "LOCATION 's3://<bucket>/iceberg/<db>/<table>/';\n",
    "```\n",
    "\n",
    "Question for later: the snowflake equivalent of `LOCATION` here is creating a stage. Why would some organizations require you to get approval before creating a stage? Is there messiness that could be caused by letting anyone create any stages they like? ðŸ¤”\n",
    "\n",
    "Question: what is Athena's equivalent of `COPY INTO`? I worry that if DS yeet files into a S3 dir and then run a glue crawler on that, there's a risk of them adding files that do not have valid schemas, therefore corrupting the whole table. Need to see how this would work.\n",
    "\n",
    "Question: how do you do data governance? How do you restrict who can access certain columns? How do you restrict who can access certain rows?\n",
    "\n",
    "Question: how do you do data documentation? How do you annotate what each column means with human-readable comments.\n",
    "\n",
    "Question: how do you see data lineage? If I have a chain of `create table ... as select ...` queries, how do we see the column-level lineage? (how is each column derived from upstream columns?)\n",
    "\n",
    "Question: how can lineage hook into OpenLineage? Can "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf9cfbcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VendorID</th>\n",
       "      <th>lpep_pickup_datetime</th>\n",
       "      <th>lpep_dropoff_datetime</th>\n",
       "      <th>RatecodeID</th>\n",
       "      <th>PULocationID</th>\n",
       "      <th>DOLocationID</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>trip_distance</th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>extra</th>\n",
       "      <th>mta_tax</th>\n",
       "      <th>tip_amount</th>\n",
       "      <th>tolls_amount</th>\n",
       "      <th>ehail_fee</th>\n",
       "      <th>improvement_surcharge</th>\n",
       "      <th>total_amount</th>\n",
       "      <th>payment_type</th>\n",
       "      <th>trip_type</th>\n",
       "      <th>congestion_surcharge</th>\n",
       "      <th>cbd_congestion_fee</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>146486.000000</td>\n",
       "      <td>146486</td>\n",
       "      <td>146486</td>\n",
       "      <td>138584.000000</td>\n",
       "      <td>146486.000000</td>\n",
       "      <td>146486.000000</td>\n",
       "      <td>138584.000000</td>\n",
       "      <td>146486.000000</td>\n",
       "      <td>146486.000000</td>\n",
       "      <td>146486.000000</td>\n",
       "      <td>146486.000000</td>\n",
       "      <td>146486.000000</td>\n",
       "      <td>146486.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>146486.000000</td>\n",
       "      <td>146486.00000</td>\n",
       "      <td>138584.000000</td>\n",
       "      <td>138560.000000</td>\n",
       "      <td>138584.000000</td>\n",
       "      <td>142662.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.924423</td>\n",
       "      <td>2025-02-15 23:22:51.800254</td>\n",
       "      <td>2025-02-15 23:43:06.287597</td>\n",
       "      <td>1.255167</td>\n",
       "      <td>95.588363</td>\n",
       "      <td>141.688455</td>\n",
       "      <td>1.281252</td>\n",
       "      <td>17.408229</td>\n",
       "      <td>17.059759</td>\n",
       "      <td>0.895360</td>\n",
       "      <td>0.601063</td>\n",
       "      <td>2.490880</td>\n",
       "      <td>0.202184</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.979542</td>\n",
       "      <td>23.18034</td>\n",
       "      <td>1.267931</td>\n",
       "      <td>1.041513</td>\n",
       "      <td>0.844509</td>\n",
       "      <td>0.063007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>2024-12-25 23:13:15</td>\n",
       "      <td>2024-12-25 23:13:17</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-470.600000</td>\n",
       "      <td>-5.000000</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>-20.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-473.10000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-2.750000</td>\n",
       "      <td>-0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>2025-01-24 20:55:54</td>\n",
       "      <td>2025-01-24 21:12:27.750000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>74.000000</td>\n",
       "      <td>74.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.110000</td>\n",
       "      <td>9.300000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>13.80000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>2025-02-15 19:17:49</td>\n",
       "      <td>2025-02-15 19:33:20</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>140.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.800000</td>\n",
       "      <td>13.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>2.020000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>18.90000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>2025-03-10 07:29:45.750000</td>\n",
       "      <td>2025-03-10 07:46:14.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>97.000000</td>\n",
       "      <td>229.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.090000</td>\n",
       "      <td>19.800000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>3.700000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>27.18000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.750000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>6.000000</td>\n",
       "      <td>2025-04-01 23:41:29</td>\n",
       "      <td>2025-04-01 23:41:50</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>265.000000</td>\n",
       "      <td>265.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>147993.110000</td>\n",
       "      <td>633.700000</td>\n",
       "      <td>12.500000</td>\n",
       "      <td>61.500000</td>\n",
       "      <td>252.050000</td>\n",
       "      <td>48.940000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>642.14000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.750000</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.580098</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.791136</td>\n",
       "      <td>55.782122</td>\n",
       "      <td>77.184062</td>\n",
       "      <td>0.933006</td>\n",
       "      <td>940.146342</td>\n",
       "      <td>14.264941</td>\n",
       "      <td>1.355397</td>\n",
       "      <td>0.391043</td>\n",
       "      <td>3.278479</td>\n",
       "      <td>1.260738</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.152176</td>\n",
       "      <td>16.48806</td>\n",
       "      <td>0.474899</td>\n",
       "      <td>0.199474</td>\n",
       "      <td>1.268505</td>\n",
       "      <td>0.208072</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            VendorID        lpep_pickup_datetime       lpep_dropoff_datetime  \\\n",
       "count  146486.000000                      146486                      146486   \n",
       "mean        1.924423  2025-02-15 23:22:51.800254  2025-02-15 23:43:06.287597   \n",
       "min         1.000000         2024-12-25 23:13:15         2024-12-25 23:13:17   \n",
       "25%         2.000000         2025-01-24 20:55:54  2025-01-24 21:12:27.750000   \n",
       "50%         2.000000         2025-02-15 19:17:49         2025-02-15 19:33:20   \n",
       "75%         2.000000  2025-03-10 07:29:45.750000  2025-03-10 07:46:14.500000   \n",
       "max         6.000000         2025-04-01 23:41:29         2025-04-01 23:41:50   \n",
       "std         0.580098                         NaN                         NaN   \n",
       "\n",
       "          RatecodeID   PULocationID   DOLocationID  passenger_count  \\\n",
       "count  138584.000000  146486.000000  146486.000000    138584.000000   \n",
       "mean        1.255167      95.588363     141.688455         1.281252   \n",
       "min         1.000000       1.000000       1.000000         0.000000   \n",
       "25%         1.000000      74.000000      74.000000         1.000000   \n",
       "50%         1.000000      75.000000     140.000000         1.000000   \n",
       "75%         1.000000      97.000000     229.000000         1.000000   \n",
       "max        99.000000     265.000000     265.000000         9.000000   \n",
       "std         2.791136      55.782122      77.184062         0.933006   \n",
       "\n",
       "       trip_distance    fare_amount          extra        mta_tax  \\\n",
       "count  146486.000000  146486.000000  146486.000000  146486.000000   \n",
       "mean       17.408229      17.059759       0.895360       0.601063   \n",
       "min         0.000000    -470.600000      -5.000000      -0.500000   \n",
       "25%         1.110000       9.300000       0.000000       0.500000   \n",
       "50%         1.800000      13.500000       0.000000       0.500000   \n",
       "75%         3.090000      19.800000       2.500000       0.500000   \n",
       "max    147993.110000     633.700000      12.500000      61.500000   \n",
       "std       940.146342      14.264941       1.355397       0.391043   \n",
       "\n",
       "          tip_amount   tolls_amount  ehail_fee  improvement_surcharge  \\\n",
       "count  146486.000000  146486.000000        0.0          146486.000000   \n",
       "mean        2.490880       0.202184        NaN               0.979542   \n",
       "min       -20.000000       0.000000        NaN              -1.000000   \n",
       "25%         0.000000       0.000000        NaN               1.000000   \n",
       "50%         2.020000       0.000000        NaN               1.000000   \n",
       "75%         3.700000       0.000000        NaN               1.000000   \n",
       "max       252.050000      48.940000        NaN               1.000000   \n",
       "std         3.278479       1.260738        NaN               0.152176   \n",
       "\n",
       "       total_amount   payment_type      trip_type  congestion_surcharge  \\\n",
       "count  146486.00000  138584.000000  138560.000000         138584.000000   \n",
       "mean       23.18034       1.267931       1.041513              0.844509   \n",
       "min      -473.10000       1.000000       1.000000             -2.750000   \n",
       "25%        13.80000       1.000000       1.000000              0.000000   \n",
       "50%        18.90000       1.000000       1.000000              0.000000   \n",
       "75%        27.18000       2.000000       1.000000              2.750000   \n",
       "max       642.14000       5.000000       2.000000              2.750000   \n",
       "std        16.48806       0.474899       0.199474              1.268505   \n",
       "\n",
       "       cbd_congestion_fee  \n",
       "count       142662.000000  \n",
       "mean             0.063007  \n",
       "min             -0.750000  \n",
       "25%              0.000000  \n",
       "50%              0.000000  \n",
       "75%              0.000000  \n",
       "max              0.750000  \n",
       "std              0.208072  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "from IPython.display import display\n",
    "from pyiceberg.partitioning import UNPARTITIONED_PARTITION_SPEC\n",
    "\n",
    "# read one of the parquet files and get its pyarrow schema\n",
    "data_files: list[str] = list(DATA_DIR.glob(\"*.parquet\"))\n",
    "\n",
    "# concat each of the df's into one parquet file to help infer the schema\n",
    "df = pd.concat(\n",
    "    [pd.read_parquet(file) for file in data_files],\n",
    "    ignore_index=True,\n",
    ")\n",
    "display(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c5266df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created Iceberg table: 'green_taxi_trips'\n",
      "Glue Table Console URL: https://us-east-1.console.aws.amazon.com/glue/home?region=us-east-1#/v2/data-catalog/tables/view/green_taxi_trips?database=nyc_taxi&catalogId=847068433460&versionId=latest&mainTab=tab-table-overview\n"
     ]
    }
   ],
   "source": [
    "arrow_df = pa.Table.from_pandas(df)\n",
    "\n",
    "# iceberg_catalog.drop_table(\n",
    "#     identifier=f\"{GLUE_DATABASE_NAME}.{GREEN_TAXI_TABLE_NAME}\",\n",
    "# )\n",
    "\n",
    "# now create an iceberg table using that schema\n",
    "iceberg_catalog.create_table_if_not_exists(\n",
    "    identifier=f\"{GLUE_DATABASE_NAME}.{GREEN_TAXI_TABLE_NAME}\",\n",
    "    schema=arrow_df.schema,\n",
    "    # partition by lpep_pickup_datetime at an hourly granularity\n",
    "    # ...\n",
    "    # I couldn't figure out how to cleanly partition by hour(lpep_pickup_datetime)\n",
    "    # the aws data wrangler library would make this much easier\n",
    "    partition_spec=UNPARTITIONED_PARTITION_SPEC,\n",
    "    # it's lame that we have to specify the location here since this would be exposed to DS ðŸ¤”\n",
    "    location=f\"s3://{S3_BUCKET_NAME}/iceberg/{GLUE_DATABASE_NAME}/{GREEN_TAXI_TABLE_NAME}/\",\n",
    ")\n",
    "\n",
    "glue_url = f\"https://us-east-1.console.aws.amazon.com/glue/home?region=us-east-1#/v2/data-catalog/tables/view/{GREEN_TAXI_TABLE_NAME}?database={GLUE_DATABASE_NAME}&catalogId=847068433460&versionId=latest&mainTab=tab-table-overview\"\n",
    "print(f\"Created Iceberg table: '{GREEN_TAXI_TABLE_NAME}'\")\n",
    "print(f\"Glue Table Console URL: {glue_url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2006d740",
   "metadata": {},
   "source": [
    "### Insert some data into the table\n",
    "\n",
    "At this point the table is empty. Let's write some data to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "848feaaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserting data file: ../../src/data/green/2025-01.parquet\n"
     ]
    }
   ],
   "source": [
    "import pyarrow as pa\n",
    "import pandas as pd\n",
    "\n",
    "data_file_fpaths: list[Path] = list(DATA_DIR.glob(\"*.parquet\"))\n",
    "\n",
    "for path in data_file_fpaths[:1]:\n",
    "    print(f\"Inserting data file: {path}\")\n",
    "    df = pd.read_parquet(path)  # drop empty columns\n",
    "    pyarrow_table = pa.Table.from_pandas(df)\n",
    "    iceberg_catalog.load_table(f\"{GLUE_DATABASE_NAME}.{GREEN_TAXI_TABLE_NAME}\").append(pyarrow_table)\n",
    "    # iceberg_catalog.load_table(f\"{GLUE_DATABASE_NAME}.{GREEN_TAXI_TABLE_NAME}\").upsert(\n",
    "    #     pyarrow_table,\n",
    "    #     join_cols=[\"lpep_pickup_datetime\", \"lpep_dropoff_datetime\", \"passenger_count\", \"trip_distance\", \"fare_amount\", \"tip_amount\", \"total_amount\"],\n",
    "    #     case_sensitive=False,\n",
    "    # ) # matt martin's sweet function\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "192f48e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lpep_pickup_datetime</th>\n",
       "      <th>lpep_dropoff_datetime</th>\n",
       "      <th>store_and_fwd_flag</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>trip_distance</th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>extra</th>\n",
       "      <th>mta_tax</th>\n",
       "      <th>tip_amount</th>\n",
       "      <th>tolls_amount</th>\n",
       "      <th>ehail_fee</th>\n",
       "      <th>improvement_surcharge</th>\n",
       "      <th>total_amount</th>\n",
       "      <th>payment_type</th>\n",
       "      <th>trip_type</th>\n",
       "      <th>congestion_surcharge</th>\n",
       "      <th>cbd_congestion_fee</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-01-01 00:03:01</td>\n",
       "      <td>2025-01-01 00:17:12</td>\n",
       "      <td>N</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.93</td>\n",
       "      <td>24.70</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>6.80</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>34.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-01-01 00:19:59</td>\n",
       "      <td>2025-01-01 00:25:52</td>\n",
       "      <td>N</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.32</td>\n",
       "      <td>8.60</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11.10</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-01-01 00:05:29</td>\n",
       "      <td>2025-01-01 00:07:21</td>\n",
       "      <td>N</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.41</td>\n",
       "      <td>25.55</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>26.55</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-01-01 00:52:24</td>\n",
       "      <td>2025-01-01 01:07:52</td>\n",
       "      <td>N</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.12</td>\n",
       "      <td>21.20</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>6.13</td>\n",
       "      <td>6.94</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>36.77</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-01-01 00:25:05</td>\n",
       "      <td>2025-01-01 01:01:10</td>\n",
       "      <td>N</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.71</td>\n",
       "      <td>33.80</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>7.81</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>46.86</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.75</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>2025-01-02 01:14:23</td>\n",
       "      <td>2025-01-02 01:21:46</td>\n",
       "      <td>N</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.66</td>\n",
       "      <td>30.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>32.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>2025-01-02 02:07:29</td>\n",
       "      <td>2025-01-02 02:07:32</td>\n",
       "      <td>N</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>20.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>21.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>2025-01-02 02:09:21</td>\n",
       "      <td>2025-01-02 02:09:23</td>\n",
       "      <td>N</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>17.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>18.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>2025-01-02 02:27:25</td>\n",
       "      <td>2025-01-02 02:27:28</td>\n",
       "      <td>N</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.07</td>\n",
       "      <td>10.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.20</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>13.20</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>2025-01-02 02:23:43</td>\n",
       "      <td>2025-01-02 02:27:44</td>\n",
       "      <td>N</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.42</td>\n",
       "      <td>30.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>33.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows Ã— 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    lpep_pickup_datetime lpep_dropoff_datetime store_and_fwd_flag  \\\n",
       "0    2025-01-01 00:03:01   2025-01-01 00:17:12                  N   \n",
       "1    2025-01-01 00:19:59   2025-01-01 00:25:52                  N   \n",
       "2    2025-01-01 00:05:29   2025-01-01 00:07:21                  N   \n",
       "3    2025-01-01 00:52:24   2025-01-01 01:07:52                  N   \n",
       "4    2025-01-01 00:25:05   2025-01-01 01:01:10                  N   \n",
       "..                   ...                   ...                ...   \n",
       "995  2025-01-02 01:14:23   2025-01-02 01:21:46                  N   \n",
       "996  2025-01-02 02:07:29   2025-01-02 02:07:32                  N   \n",
       "997  2025-01-02 02:09:21   2025-01-02 02:09:23                  N   \n",
       "998  2025-01-02 02:27:25   2025-01-02 02:27:28                  N   \n",
       "999  2025-01-02 02:23:43   2025-01-02 02:27:44                  N   \n",
       "\n",
       "     passenger_count  trip_distance  fare_amount  extra  mta_tax  tip_amount  \\\n",
       "0                1.0           5.93        24.70    1.0      0.5        6.80   \n",
       "1                1.0           1.32         8.60    1.0      0.5        0.00   \n",
       "2                1.0           0.41        25.55    0.0      0.0        0.00   \n",
       "3                1.0           4.12        21.20    1.0      0.5        6.13   \n",
       "4                1.0           4.71        33.80    1.0      0.5        7.81   \n",
       "..               ...            ...          ...    ...      ...         ...   \n",
       "995              1.0           1.66        30.00    0.0      0.0        1.00   \n",
       "996              1.0           0.00        20.00    0.0      0.0        0.00   \n",
       "997              1.0           0.00        17.00    0.0      0.0        0.00   \n",
       "998              1.0           0.07        10.00    0.0      0.0        2.20   \n",
       "999              1.0           0.42        30.00    0.0      0.0        2.00   \n",
       "\n",
       "     tolls_amount  ehail_fee  improvement_surcharge  total_amount  \\\n",
       "0            0.00        NaN                    1.0         34.00   \n",
       "1            0.00        NaN                    1.0         11.10   \n",
       "2            0.00        NaN                    1.0         26.55   \n",
       "3            6.94        NaN                    1.0         36.77   \n",
       "4            0.00        NaN                    1.0         46.86   \n",
       "..            ...        ...                    ...           ...   \n",
       "995          0.00        NaN                    1.0         32.00   \n",
       "996          0.00        NaN                    1.0         21.00   \n",
       "997          0.00        NaN                    1.0         18.00   \n",
       "998          0.00        NaN                    1.0         13.20   \n",
       "999          0.00        NaN                    1.0         33.00   \n",
       "\n",
       "     payment_type  trip_type  congestion_surcharge  cbd_congestion_fee  \n",
       "0             1.0        1.0                  0.00                 0.0  \n",
       "1             2.0        1.0                  0.00                 0.0  \n",
       "2             2.0        2.0                  0.00                 0.0  \n",
       "3             1.0        1.0                  0.00                 0.0  \n",
       "4             1.0        1.0                  2.75                 0.0  \n",
       "..            ...        ...                   ...                 ...  \n",
       "995           1.0        2.0                  0.00                 0.0  \n",
       "996           1.0        2.0                  0.00                 0.0  \n",
       "997           1.0        2.0                  0.00                 0.0  \n",
       "998           1.0        2.0                  0.00                 0.0  \n",
       "999           1.0        2.0                  0.00                 0.0  \n",
       "\n",
       "[1000 rows x 17 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# validate it!\n",
    "import awswrangler as wr\n",
    "import boto3\n",
    "\n",
    "# query = \"\"\"\\\n",
    "# SELECT VendorID, AVG(trip_distance) AS avg_dist\n",
    "# FROM AwsDataCatalog.nyc_taxi.green_taxi_trips\n",
    "# WHERE passenger_count > 1\n",
    "# GROUP BY VendorID;\n",
    "# \"\"\"\n",
    "\n",
    "query = \"\"\"\\\n",
    "SELECT *\n",
    "FROM AwsDataCatalog.nyc_taxi.green_taxi_trips\n",
    "WHERE lpep_pickup_datetime > TIMESTAMP '2024-12-01 00:00:00'\n",
    "LIMIT 1000\n",
    "\"\"\"\n",
    "\n",
    "boto3.setup_default_session(region_name=AWS_REGION)\n",
    "wr.athena.read_sql_query(\n",
    "    sql=query,\n",
    "    database=\"nyc\",\n",
    "    ctas_approach=False,\n",
    "    boto3_session=boto3.Session(region_name=AWS_REGION)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b80d2a8",
   "metadata": {},
   "source": [
    "## Attempt to connect via duckDB\n",
    "\n",
    "Using the method documented here: https://duckdb.org/docs/stable/core_extensions/iceberg/amazon_sagemaker_lakehouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dd015f7b",
   "metadata": {},
   "outputs": [
    {
     "ename": "HTTPException",
     "evalue": "HTTP Error: Failed to query https://glue.us-east-1.amazonaws.com/iceberg/v1/config?warehouse=792808862870%3As3tablescatalog%2Fnyc_taxi, http error 403 thrown. Message: {\"message\":\"The security token included in the request is invalid.\"}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mHTTPException\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     10\u001b[39m con = duckdb.connect()\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# Attach to AWS Glue Iceberg REST catalog using explicit REST endpoint and SigV4 options\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[43mcon\u001b[49m\u001b[43m.\u001b[49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\"\"\u001b[39;49m\n\u001b[32m     14\u001b[39m \u001b[33;43mINSTALL iceberg;\u001b[39;49m\n\u001b[32m     15\u001b[39m \u001b[33;43mLOAD iceberg;\u001b[39;49m\n\u001b[32m     16\u001b[39m \u001b[33;43mCREATE SECRET glue_iceberg_secret (\u001b[39;49m\n\u001b[32m     17\u001b[39m \u001b[33;43m    TYPE s3,\u001b[39;49m\n\u001b[32m     18\u001b[39m \u001b[33;43m    PROVIDER credential_chain,\u001b[39;49m\n\u001b[32m     19\u001b[39m \u001b[33;43m    CHAIN sts,\u001b[39;49m\n\u001b[32m     20\u001b[39m \u001b[33;43m    REGION \u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mAWS_REGION\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m'\u001b[39;49m\u001b[33;43m,\u001b[39;49m\n\u001b[32m     21\u001b[39m \u001b[33;43m    PROFILE \u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mAWS_PROFILE\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m'\u001b[39;49m\n\u001b[32m     22\u001b[39m \u001b[33;43m);\u001b[39;49m\n\u001b[32m     23\u001b[39m \n\u001b[32m     24\u001b[39m \u001b[33;43mATTACH \u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43maccount_id\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m:s3tablescatalog/nyc_taxi\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m AS glue_catalog (\u001b[39;49m\n\u001b[32m     25\u001b[39m \u001b[33;43m    TYPE iceberg,\u001b[39;49m\n\u001b[32m     26\u001b[39m \u001b[33;43m    ENDPOINT_TYPE glue\u001b[39;49m\n\u001b[32m     27\u001b[39m \u001b[33;43m);\u001b[39;49m\n\u001b[32m     28\u001b[39m \u001b[33;43m\"\"\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[38;5;28mprint\u001b[39m(con.execute(\u001b[33m\"\u001b[39m\u001b[33mSHOW ALL TABLES;\u001b[39m\u001b[33m\"\u001b[39m).fetchdf())\n",
      "\u001b[31mHTTPException\u001b[39m: HTTP Error: Failed to query https://glue.us-east-1.amazonaws.com/iceberg/v1/config?warehouse=792808862870%3As3tablescatalog%2Fnyc_taxi, http error 403 thrown. Message: {\"message\":\"The security token included in the request is invalid.\"}"
     ]
    }
   ],
   "source": [
    "# duckdb\n",
    "\n",
    "import duckdb\n",
    "import boto3\n",
    "\n",
    "# Get AWS account ID for the ATTACH statement\n",
    "sts = boto3.client(\"sts\")\n",
    "account_id = sts.get_caller_identity()[\"Account\"]\n",
    "\n",
    "con = duckdb.connect()\n",
    "\n",
    "# Attach to AWS Glue Iceberg REST catalog using explicit REST endpoint and SigV4 options\n",
    "con.sql(f\"\"\"\n",
    "INSTALL iceberg;\n",
    "LOAD iceberg;\n",
    "CREATE SECRET glue_iceberg_secret (\n",
    "    TYPE s3,\n",
    "    PROVIDER credential_chain,\n",
    "    CHAIN sts,\n",
    "    REGION '{AWS_REGION}',\n",
    "    PROFILE '{AWS_PROFILE}'\n",
    ");\n",
    "\n",
    "ATTACH '{account_id}:s3tablescatalog/nyc_taxi' AS glue_catalog (\n",
    "    TYPE iceberg,\n",
    "    ENDPOINT_TYPE glue\n",
    ");\n",
    "\"\"\")\n",
    "\n",
    "print(con.execute(\"SHOW ALL TABLES;\").fetchdf())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94982dbb",
   "metadata": {},
   "source": [
    "## Another attempt to connect using the REST Catalog interface\n",
    "\n",
    "Docs here: https://duckdb.org/docs/stable/core_extensions/iceberg/iceberg_rest_catalogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e48f3e51",
   "metadata": {},
   "outputs": [
    {
     "ename": "HTTPException",
     "evalue": "HTTP Error: Failed to query https://glue.us-east-1.amazonaws.com/iceberg/v1/config?warehouse=792808862870%3Anyc_taxi, http error 403 thrown. Message: {\"message\":\"The security token included in the request is invalid.\"}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mHTTPException\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     10\u001b[39m con = duckdb.connect()\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# Attach to AWS Glue Iceberg REST catalog using explicit REST endpoint and SigV4 options\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m \u001b[43mcon\u001b[49m\u001b[43m.\u001b[49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\"\"\u001b[39;49m\n\u001b[32m     15\u001b[39m \u001b[33;43mINSTALL iceberg;\u001b[39;49m\n\u001b[32m     16\u001b[39m \u001b[33;43mLOAD iceberg;\u001b[39;49m\n\u001b[32m     17\u001b[39m \u001b[33;43mCREATE SECRET glue_iceberg_secret (\u001b[39;49m\n\u001b[32m     18\u001b[39m \u001b[33;43m    TYPE s3,\u001b[39;49m\n\u001b[32m     19\u001b[39m \u001b[33;43m    PROVIDER credential_chain,\u001b[39;49m\n\u001b[32m     20\u001b[39m \u001b[33;43m    CHAIN sts,\u001b[39;49m\n\u001b[32m     21\u001b[39m \u001b[33;43m    REGION \u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mAWS_REGION\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m'\u001b[39;49m\u001b[33;43m,\u001b[39;49m\n\u001b[32m     22\u001b[39m \u001b[33;43m    PROFILE \u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mAWS_PROFILE\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m'\u001b[39;49m\n\u001b[32m     23\u001b[39m \u001b[33;43m);\u001b[39;49m\n\u001b[32m     24\u001b[39m \n\u001b[32m     25\u001b[39m \u001b[33;43mATTACH \u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43maccount_id\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m:nyc_taxi\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m AS glue_catalog (\u001b[39;49m\n\u001b[32m     26\u001b[39m \u001b[33;43m    TYPE iceberg,\u001b[39;49m\n\u001b[32m     27\u001b[39m \u001b[33;43m    ENDPOINT_TYPE \u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mGLUE\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m,\u001b[39;49m\n\u001b[32m     28\u001b[39m \u001b[33;43m    ENDPOINT \u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mhttps://glue.\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mAWS_REGION\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m.amazonaws.com/iceberg\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m,\u001b[39;49m\n\u001b[32m     29\u001b[39m \u001b[33;43m    SECRET glue_iceberg_secret\u001b[39;49m\n\u001b[32m     30\u001b[39m \u001b[33;43m);\u001b[39;49m\n\u001b[32m     31\u001b[39m \u001b[33;43m\"\"\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[38;5;28mprint\u001b[39m(con.execute(\u001b[33m\"\u001b[39m\u001b[33mSHOW ALL TABLES;\u001b[39m\u001b[33m\"\u001b[39m).fetchdf())\n",
      "\u001b[31mHTTPException\u001b[39m: HTTP Error: Failed to query https://glue.us-east-1.amazonaws.com/iceberg/v1/config?warehouse=792808862870%3Anyc_taxi, http error 403 thrown. Message: {\"message\":\"The security token included in the request is invalid.\"}"
     ]
    }
   ],
   "source": [
    "# duckdb\n",
    "\n",
    "import duckdb\n",
    "import boto3\n",
    "\n",
    "# Get AWS account ID for the ATTACH statement\n",
    "sts = boto3.client(\"sts\")\n",
    "account_id = sts.get_caller_identity()[\"Account\"]\n",
    "\n",
    "con = duckdb.connect()\n",
    "\n",
    "\n",
    "# Attach to AWS Glue Iceberg REST catalog using explicit REST endpoint and SigV4 options\n",
    "con.sql(f\"\"\"\n",
    "INSTALL iceberg;\n",
    "LOAD iceberg;\n",
    "CREATE SECRET glue_iceberg_secret (\n",
    "    TYPE s3,\n",
    "    PROVIDER credential_chain,\n",
    "    CHAIN sts,\n",
    "    REGION '{AWS_REGION}',\n",
    "    PROFILE '{AWS_PROFILE}'\n",
    ");\n",
    "\n",
    "ATTACH '{account_id}:nyc_taxi' AS glue_catalog (\n",
    "    TYPE iceberg,\n",
    "    ENDPOINT_TYPE 'GLUE',\n",
    "    ENDPOINT 'https://glue.{AWS_REGION}.amazonaws.com/iceberg',\n",
    "    SECRET glue_iceberg_secret\n",
    ");\n",
    "\"\"\")\n",
    "\n",
    "print(con.execute(\"SHOW ALL TABLES;\").fetchdf())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05fc8e26",
   "metadata": {},
   "source": [
    "## Another attempt to connect\n",
    "\n",
    "Our bucket is not an S3 tables bucket, so possibly we need to specify the database name in the connection string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "93ca019d",
   "metadata": {},
   "outputs": [
    {
     "ename": "HTTPException",
     "evalue": "HTTP Error: Failed to query https://glue.us-east-1.amazonaws.com/iceberg/v1/config?warehouse=792808862870%3Anyc_taxi, http error 403 thrown. Message: {\"message\":\"The security token included in the request is invalid.\"}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mHTTPException\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      8\u001b[39m con = duckdb.connect()\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Attach to AWS Glue Iceberg REST catalog using explicit REST endpoint and SigV4 options\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[43mcon\u001b[49m\u001b[43m.\u001b[49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\"\"\u001b[39;49m\n\u001b[32m     13\u001b[39m \u001b[33;43mINSTALL iceberg;\u001b[39;49m\n\u001b[32m     14\u001b[39m \u001b[33;43mLOAD iceberg;\u001b[39;49m\n\u001b[32m     15\u001b[39m \u001b[33;43mCREATE SECRET glue_iceberg_secret (\u001b[39;49m\n\u001b[32m     16\u001b[39m \u001b[33;43m    TYPE s3,\u001b[39;49m\n\u001b[32m     17\u001b[39m \u001b[33;43m    PROVIDER credential_chain,\u001b[39;49m\n\u001b[32m     18\u001b[39m \u001b[33;43m    CHAIN sts,\u001b[39;49m\n\u001b[32m     19\u001b[39m \u001b[33;43m    REGION \u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mAWS_REGION\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m'\u001b[39;49m\u001b[33;43m,\u001b[39;49m\n\u001b[32m     20\u001b[39m \u001b[33;43m    PROFILE \u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mAWS_PROFILE\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m'\u001b[39;49m\n\u001b[32m     21\u001b[39m \u001b[33;43m);\u001b[39;49m\n\u001b[32m     22\u001b[39m \n\u001b[32m     23\u001b[39m \u001b[33;43mATTACH \u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43maccount_id\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m:nyc_taxi\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m AS glue_catalog (\u001b[39;49m\n\u001b[32m     24\u001b[39m \u001b[33;43m    TYPE iceberg,\u001b[39;49m\n\u001b[32m     25\u001b[39m \u001b[33;43m    ENDPOINT_TYPE glue\u001b[39;49m\n\u001b[32m     26\u001b[39m \u001b[33;43m);\u001b[39;49m\n\u001b[32m     27\u001b[39m \u001b[33;43m\"\"\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[38;5;28mprint\u001b[39m(con.execute(\u001b[33m\"\u001b[39m\u001b[33mSHOW ALL TABLES;\u001b[39m\u001b[33m\"\u001b[39m).fetchdf())\n",
      "\u001b[31mHTTPException\u001b[39m: HTTP Error: Failed to query https://glue.us-east-1.amazonaws.com/iceberg/v1/config?warehouse=792808862870%3Anyc_taxi, http error 403 thrown. Message: {\"message\":\"The security token included in the request is invalid.\"}"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "import boto3\n",
    "\n",
    "# Get AWS account ID for the ATTACH statement\n",
    "sts = boto3.client(\"sts\")\n",
    "account_id = sts.get_caller_identity()[\"Account\"]\n",
    "\n",
    "con = duckdb.connect()\n",
    "\n",
    "\n",
    "# Attach to AWS Glue Iceberg REST catalog using explicit REST endpoint and SigV4 options\n",
    "con.sql(f\"\"\"\n",
    "INSTALL iceberg;\n",
    "LOAD iceberg;\n",
    "CREATE SECRET glue_iceberg_secret (\n",
    "    TYPE s3,\n",
    "    PROVIDER credential_chain,\n",
    "    CHAIN sts,\n",
    "    REGION '{AWS_REGION}',\n",
    "    PROFILE '{AWS_PROFILE}'\n",
    ");\n",
    "\n",
    "ATTACH '{account_id}:nyc_taxi' AS glue_catalog (\n",
    "    TYPE iceberg,\n",
    "    ENDPOINT_TYPE glue\n",
    ");\n",
    "\"\"\")\n",
    "\n",
    "print(con.execute(\"SHOW ALL TABLES;\").fetchdf())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e868f6",
   "metadata": {},
   "source": [
    "# Cleanup created resources\n",
    "\n",
    "See. There was a reason for Pulumi!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a099f25e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DestroyResult(stdout='Destroying (dev):\\n\\n\\n\\n@ destroying.....\\n\\n -  aws:glue:CatalogDatabase glue-db deleting (0s) \\n\\n -  aws:s3:Bucket datalake-bucket deleting (0s) \\n\\n@ destroying....\\n\\n -  aws:glue:CatalogDatabase glue-db deleted (1s) \\n\\n@ destroying.....\\n\\n -  aws:s3:Bucket datalake-bucket deleted (3s) \\n\\n -  pulumi:pulumi:Stack s3-bucket-project-dev deleting (0s) \\n\\n -  pulumi:pulumi:Stack s3-bucket-project-dev deleted (0.00s) \\n\\nOutputs:\\n\\n  - bucket_name       : \"mlops-club-datalake-stream\"\\n\\n  - glue_database_name: \"nyc_taxi\"\\n\\n\\n\\nResources:\\n\\n    - 3 deleted\\n\\n\\n\\nDuration: 5s\\n\\n\\n\\nThe resources in the stack have been deleted, but the history and configuration associated with the stack are still maintained. \\n\\nIf you want to remove the stack completely, run `pulumi stack rm dev`.\\n', stderr='', summary=UpdateSummary(result='succeeded', version=0, start_time=datetime.datetime(2025, 6, 8, 19, 30, 8), end_time=datetime.datetime(2025, 6, 8, 19, 30, 13), kind='destroy', message='', environment={'exec.kind': 'auto.inline', 'pulumi.arch': 'arm64', 'pulumi.env.PULUMI_CONFIG_PASSPHRASE': 'set', 'pulumi.env.PULUMI_DEBUG_COMMANDS': 'true', 'pulumi.flag.exec-kind': 'set', 'pulumi.flag.non-interactive': 'true', 'pulumi.flag.skip-preview': 'true', 'pulumi.flag.stack': 'set', 'pulumi.flag.yes': 'true', 'pulumi.os': 'darwin', 'pulumi.version': 'v3.162.0', 'stack.environments': '[]', 'updatePlan': 'false'}, resource_changes={'delete': 3}, config={'aws:region': 'us-east-1'}, Deployment=None))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cleanup the stack\n",
    "stack.destroy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5532451",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c97fef3",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
